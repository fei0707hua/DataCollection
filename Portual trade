import requests
import csv
import os
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil.relativedelta import relativedelta
from concurrent.futures import ThreadPoolExecutor

ctr1 = 'MUNDO,INTRA_UE27_2020,EXTRA_UE27_2020,EFTA,OPEP,PALOP,AD,AE,AF,AG,AI,AL,AM,AO,AQ,AR,AS,AT,AU,AW,AZ,BA,BB,BD,BE,BF,BG,BH,BI,BJ,BM,BN,BO,BR,BS,BT,BV,BW,BY,BZ,CA,CC,CD,CF,CG,CH,CI,CK,CL,CM,CN,CO,CR,CU,CV,CX,CY,CZ,DE,DJ,DK,DM,DO,DZ,EC,EE,EG,ER,ES,ET,FI,FJ,FK,FM,FO,FR,GA,GD,GE,GH,GI,GL,GM,GN,GQ,GR,GS,GT,GU,GW,GY,HK,HM,HN,HR,HT,HU,ID,IE,IL,IN,IO,IQ,IR,IS,IT,JM,JO,JP,KE,KG,KH,KI,KM,KN,KP,KR,KW,KY,KZ'
ctr2 = 'LA,LB,LC,LI,LK,LR,LS,LT,LU,LV,LY,MA,MD,ME,MG,MH,MK,ML,MM,MN,MO,MP,MR,MS,MT,MU,MV,MW,MX,MY,MZ,NA,NC,NE,NF,NG,NI,NL,NO,NP,NR,NU,NZ,OM,PA,PE,PF,PG,PH,PK,PL,PM,PN,PS,PW,PY,QA,RO,RU,RW,SA,SB,SC,SD,SE,SG,SH,SI,SK,SL,SM,SN,SO,SR,ST,SV,SY,SZ,TC,TD,TF,TG,TH,TJ,TK,TL,TM,TN,TO,TR,TT,TV,TW,TZ,UA,UG,UM,US,UY,UZ,VA,VC,VE,VG,VI,VN,VU,WF,WS,XC,XK,XL,XS,YE,YT,ZA,ZM,ZW,XU,XI'
goods = 'T,I,01,02,03,04,05,II,06,07,08,09,10,11,12,13,III,14,15,IV,16,17,18,19,20,21,22,23,V,24,25,26,27,VI,28,29,30,31,32,33,34,35,36,37,38,VII,39,VIII,40,41,IX,42,43,44,X,45,46,47,48,XI,49,50,51,52,53,54,55,56,57,58,59,60,61,62,XII,XIII,63,64,65,66,67,68,XIV,69,XV,70,71,72,73,74,75,76,78,XVI,79,80,81,82,83,84,85,XVII,XVIII,86,87,88,89,XIX,90,XX,XXI,91,92,93,94,95,96,97,98,99'

endpoint = 'https://www.ine.pt/ine/json_indicador/pindica.jsp?op=2'
duration = 1

def get_current_date(varcd):
    url = f'https://www.ine.pt/ine/xml_indic.jsp?opc=1&varcd={varcd}&lang=EN'
    page = requests.get(url)
    soup = BeautifulSoup(page.text, 'xml')
    cur_date_code = soup.find('last_period_available').text
    return cur_date_code

def get_date_2yr(cur_date_code):
    cur_date = cur_date_code[3:]
    end_date = datetime.strptime(cur_date, '%Y%m')
    start_date = end_date - relativedelta(months=24)
    list_date = []
    while start_date <= end_date:
        list_date.append('S3A' + start_date.strftime('%Y%m'))
        start_date += relativedelta(months=1)
    return list_date

def get_date_2019(cur_date_code):
    cur_date = cur_date_code[3:]
    end_date = datetime.strptime(cur_date, '%Y%m')
    start_date = datetime.strptime('201910', '%Y%m')
    list_date = []
    while start_date <= end_date:
        list_date.append('S3A' + start_date.strftime('%Y%m'))
        start_date += relativedelta(months=1)
    return list_date

def fetch_process_save(varcd, dim1_list, dim2, dim3, filename):
    for i in range(0, len(dim1_list), duration):
        chunk = dim1_list[i:i + duration]
        period_str = ','.join(chunk).replace('S3A', '')
        params = {
            'varcd': varcd,
            'Dim1': ','.join(chunk),
            'Dim2': dim2,
            'Dim3': dim3,
            'lang': 'EN'
        }

        print(f"Fetching {filename} for period: {period_str}")
        response = requests.get(endpoint, params=params)
        json_data = response.json()
        data = json_data[0].get('Dados')

        if data:
            rows = process_json_data(data)
            save_as_csv(filename, rows)
            print(f"Saved data for {period_str} in {filename}.csv")

def process_json_data(data):
    rows = []
    for date, records in data.items():
        for record in records:
            rows.append({
                'Date': date,
                'Code': record['geocod'] + record['dim_3'],
                'Destination': record['geodsg'],
                'Category': record['dim_3_t'],
                'Value': float(record.get('valor', None))
            })
    return rows

def save_as_csv(filename, rows):
    file_path = f"{filename}.csv"
    file_exists = os.path.isfile(file_path)

    with open(file_path, mode='a', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=rows[0].keys())
        if not file_exists:
            writer.writeheader()
        writer.writerows(rows)

# Main function for one pipeline
def run_pipeline(varcd, dim1_func, dim2, dim3, filename):
    cur_date_code = get_current_date(varcd)
    dim1_list = dim1_func(cur_date_code)
    fetch_process_save(varcd, dim1_list, dim2, dim3, filename)

# Multiple pipelines run in parallel
def run_all_pipelines():
    pipelines = [
        {
            'varcd': '0001400',
            'dim1_func': get_date_2019,
            'dim2': 'MUNDO,INTRA_UE27_2020,EXTRA_UE27_2020',
            'dim3': 'T,1,11,12,2,21,22,3,31,32,4,41,42,5,51,52,53,6,61,62,63,7',  
            'filename' : 'exp'
        },

        {
            'varcd': '0005716',
            'dim1_func': get_date_2yr,
            'dim2': 'MUNDO',
            'dim3': goods,
            'filename': 'exp2'
        },
        
        {
            'varcd': '0005716',
            'dim1_func': get_date_2yr,
            'dim2': 'INTRA_UE27_2020',
            'dim3': goods,
            'filename': 'exp2'
        },
        
        {
            'varcd': '0005716',
            'dim1_func': get_date_2yr,
            'dim2': 'EXTRA_UE27_2020',
            'dim3': goods,
            'filename': 'exp2'
        },

        {
            'varcd': '0004172',
            'dim1_func': get_date_2019,
            'dim2': 'MUNDO',
            'dim3': 'T,01,02,03,05,06,07,08,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,35,37,38,45,46,58,59,71,74,90,91,96',  
            'filename': 'exp3'
        },

        {
            'varcd': '0001400',
            'dim1_func': get_date_2019,
            'dim2': ctr1,
            'dim3': 'T',  
            'filename': 'exp4'
        },
        
        {
            'varcd': '0001400',
            'dim1_func': get_date_2019,
            'dim2': ctr2,
            'dim3': 'T',  
            'filename': 'exp4'
        },

        {
            'varcd': '0001397',
            'dim1_func': get_date_2019,
            'dim2': 'MUNDO,INTRA_UE27_2020,EXTRA_UE27_2020',
            'dim3': 'T,1,11,12,2,21,22,3,31,32,4,41,42,5,51,52,53,6,61,62,63,7',  
            'filename': 'imp'
        },

        {
            'varcd': '0005715',
            'dim1_func': get_date_2yr,
            'dim2': 'MUNDO',
            'dim3': goods,
            'filename': 'imp2'
        } ,
        
        {
            'varcd': '0005715',
            'dim1_func': get_date_2yr,
            'dim2': 'INTRA_UE27_2020',
            'dim3': goods,
            'filename': 'imp2'
        } ,
        
        {
            'varcd': '0005715',
            'dim1_func': get_date_2yr,
            'dim2': 'EXTRA_UE27_2020',
            'dim3': goods,
            'filename': 'imp2'
        } ,


        {
            'varcd': '0004169',
            'dim1_func': get_date_2019,
            'dim2': 'MUNDO',
            'dim3': 'T,01,02,03,05,06,07,08,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,35,37,38,45,46,58,59,71,74,90,91,96',  
            'filename': 'imp3'
        },
        
        {
            'varcd': '0001397',
            'dim1_func': get_date_2019,
            'dim2': ctr1,
            'dim3': 'T',  
            'filename': 'imp4'
        },
        
        {
            'varcd': '0001397',
            'dim1_func': get_date_2019,
            'dim2': ctr2,
            'dim3': 'T',  
            'filename': 'imp4'
        }
    ]

    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(run_pipeline, **pipe) for pipe in pipelines]
        for future in futures:
            future.result()

# Run it all
if __name__ == "__main__":
    run_all_pipelines()
    
print('all files are downloaded')
